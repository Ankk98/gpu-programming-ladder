{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Programming Ladder URL Validation System\n",
    "\n",
    "This notebook orchestrates an AI agent system to validate and update URLs in the GPU Programming Ladder data.\n",
    "\n",
    "## Features\n",
    "- **Multi-agent system**: Task creator agent + multiple consumer agents\n",
    "- **URL validation**: Checks if URLs exist and are accessible\n",
    "- **Content analysis**: Ensures exercise/video URLs point to specific content, not listings\n",
    "- **AI-powered replacements**: Uses local LLM (GPT-4o via LM Studio) to find replacements\n",
    "- **Parallel processing**: Configurable concurrent requests with rate limiting\n",
    "- **Thread safety**: No race conditions in concurrent operations\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- LM Studio running locally with GPT-4o model\n",
    "- Required Python packages (installed in virtual environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path for imports\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our custom modules\n",
    "from url_validation_orchestrator import URLValidationOrchestrator\n",
    "from url_extractor import extract_urls_from_data_js\n",
    "from task_creator_agent import TaskCreatorAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure the validation system parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  lm_studio_url: http://localhost:1234\n",
      "  num_consumer_agents: 2\n",
      "  max_concurrent_requests_per_agent: 2\n",
      "  data_js_file: ../data.js\n",
      "  urls_file: urls_to_validate.json\n",
      "  tasks_file: validation_tasks.json\n",
      "  results_file: validation_results.json\n",
      "  update_data_js: True\n",
      "  force_revalidation: False\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json file.\"\"\"\n",
    "    try:\n",
    "        with open(\"config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Convert config structure to match notebook expectations\n",
    "        return {\n",
    "            \"lm_studio_url\": config[\"lm_studio\"][\"url\"],\n",
    "            \"num_consumer_agents\": config[\"agents\"][\"num_consumer_agents\"],\n",
    "            \"max_concurrent_requests_per_agent\": config[\"agents\"][\"max_concurrent_requests_per_agent\"],\n",
    "            \"data_js_file\": config[\"files\"][\"data_js\"],\n",
    "            \"urls_file\": config[\"files\"][\"urls_to_validate\"],\n",
    "            \"tasks_file\": config[\"files\"][\"validation_tasks\"],\n",
    "            \"results_file\": config[\"files\"][\"validation_results\"],\n",
    "            \"update_data_js\": config[\"processing\"][\"update_data_js\"],\n",
    "            \"force_revalidation\": config[\"processing\"][\"force_revalidation\"]\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå config.json not found. Using default configuration.\")\n",
    "        return {\n",
    "            \"lm_studio_url\": \"http://localhost:1234\",\n",
    "            \"num_consumer_agents\": 2,\n",
    "            \"max_concurrent_requests_per_agent\": 2,\n",
    "            \"data_js_file\": \"../data.js\",\n",
    "            \"urls_file\": \"urls_to_validate.json\",\n",
    "            \"tasks_file\": \"validation_tasks.json\",\n",
    "            \"results_file\": \"validation_results.json\",\n",
    "            \"update_data_js\": True,\n",
    "            \"force_revalidation\": False\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading config.json: {e}. Using default configuration.\")\n",
    "        return {\n",
    "            \"lm_studio_url\": \"http://localhost:1234\",\n",
    "            \"num_consumer_agents\": 2,\n",
    "            \"max_concurrent_requests_per_agent\": 2,\n",
    "            \"data_js_file\": \"../data.js\",\n",
    "            \"urls_file\": \"urls_to_validate.json\",\n",
    "            \"tasks_file\": \"validation_tasks.json\",\n",
    "            \"results_file\": \"validation_results.json\",\n",
    "            \"update_data_js\": True,\n",
    "            \"force_revalidation\": False\n",
    "        }\n",
    "\n",
    "# Load configuration\n",
    "CONFIG = load_config()\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract URLs from data.js\n",
    "\n",
    "First, let's extract all URLs from the data.js file that need validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting URLs from data.js...\n",
      "‚úÖ Extracted 260 URLs\n",
      "\n",
      "üìä URL distribution by type:\n",
      "   article: 62\n",
      "   paper: 26\n",
      "   video: 61\n",
      "   exercise: 65\n",
      "   python: 37\n",
      "   cpp: 9\n",
      "\n",
      "üìã Sample URLs to validate:\n",
      "   1. [article] CPU vs GPU: Why GPUs for ML and HPC...\n",
      "      URL: https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
      "   2. [paper] CPU vs GPU: Why GPUs for ML and HPC...\n",
      "      URL: https://dl.acm.org/doi/10.1145/1365490.1365500\n",
      "   3. [video] CPU vs GPU: Why GPUs for ML and HPC...\n",
      "      URL: https://www.youtube.com/watch?v=-P28LKWTzrI\n",
      "   4. [exercise] CPU vs GPU: Why GPUs for ML and HPC...\n",
      "      URL: https://leetgpu.com/challenges\n",
      "   5. [article] GPU Architecture: SMs, warps, cores...\n",
      "      URL: https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/kernel_sm\n"
     ]
    }
   ],
   "source": [
    "# Extract URLs from data.js\n",
    "print(\"üîç Extracting URLs from data.js...\")\n",
    "\n",
    "try:\n",
    "    urls = extract_urls_from_data_js(CONFIG['data_js_file'])\n",
    "    print(f\"‚úÖ Extracted {len(urls)} URLs\")\n",
    "    \n",
    "    # Show URL type distribution\n",
    "    url_types = {}\n",
    "    for url in urls:\n",
    "        url_type = url['url_type']\n",
    "        url_types[url_type] = url_types.get(url_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìä URL distribution by type:\")\n",
    "    for url_type, count in url_types.items():\n",
    "        print(f\"   {url_type}: {count}\")\n",
    "    \n",
    "    # Show sample URLs\n",
    "    print(\"\\nüìã Sample URLs to validate:\")\n",
    "    for i, url in enumerate(urls[:5]):\n",
    "        print(f\"   {i+1}. [{url['url_type']}] {url['topic_title'][:50]}...\")\n",
    "        print(f\"      URL: {url['url']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error extracting URLs: {e}\")\n",
    "    urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Validation Tasks\n",
    "\n",
    "Create validation tasks for URLs that haven't been validated yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Creating validation tasks...\n",
      "üîÑ Task Creator Agent starting...\n",
      "üìã Loaded 260 URLs from urls_to_validate.json\n",
      "‚úÖ Created 260 validation tasks\n",
      "üíæ Saved tasks to validation_tasks.json\n",
      "\n",
      "üìä Task Summary:\n",
      "   Total tasks: 260\n",
      "   Tasks by type:\n",
      "     article: 62\n",
      "     paper: 26\n",
      "     video: 61\n",
      "     exercise: 65\n",
      "     python: 37\n",
      "     cpp: 9\n",
      "\n",
      "‚úÖ Created 260 validation tasks\n",
      "\n",
      "üìä Tasks by URL type:\n",
      "   article: 62\n",
      "   paper: 26\n",
      "   video: 61\n",
      "   exercise: 65\n",
      "   python: 37\n",
      "   cpp: 9\n"
     ]
    }
   ],
   "source": [
    "# Create validation tasks\n",
    "print(\"üìã Creating validation tasks...\")\n",
    "\n",
    "try:\n",
    "    task_creator = TaskCreatorAgent(CONFIG['urls_file'])\n",
    "    \n",
    "    # Run task creation\n",
    "    tasks = await task_creator.run()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {len(tasks)} validation tasks\")\n",
    "    \n",
    "    # Show task distribution\n",
    "    task_types = {}\n",
    "    for task in tasks:\n",
    "        url_type = task['url_entry']['url_type']\n",
    "        task_types[url_type] = task_types.get(url_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìä Tasks by URL type:\")\n",
    "    for url_type, count in task_types.items():\n",
    "        print(f\"   {url_type}: {count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating tasks: {e}\")\n",
    "    tasks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run URL Validation\n",
    "\n",
    "Run the multi-agent URL validation system. This will:\n",
    "- Check if URLs exist and are accessible\n",
    "- Analyze content to ensure appropriate targeting\n",
    "- Use AI to find replacements for broken/inappropriate URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:12,452 - URLValidatorAgent_agent_1 - INFO - Agent agent_1 initialized with log file: url_validation_agent_agent_1_20251223_153312.log\n",
      "2025-12-23 15:33:12,452 - URLValidatorAgent_agent_1 - INFO - Starting to process 130 tasks\n",
      "2025-12-23 15:33:12,453 - URLValidatorAgent_agent_1 - INFO - [e177a8d8-b909-4d79-8ac5-9adf3c1d9b9d] Processing task 1/130: https://developer.nvidia.com/blog/even-easier-introduction-cuda/ (type: article)\n",
      "2025-12-23 15:33:12,453 - URLValidatorAgent_agent_1 - INFO - Starting generic content analysis\n",
      "2025-12-23 15:33:12,453 - URLValidatorAgent_agent_1 - INFO - Starting iteration 1/10\n",
      "2025-12-23 15:33:12,454 - URLValidatorAgent_agent_2 - INFO - Agent agent_2 initialized with log file: url_validation_agent_agent_2_20251223_153312.log\n",
      "2025-12-23 15:33:12,455 - URLValidatorAgent_agent_2 - INFO - Starting to process 130 tasks\n",
      "2025-12-23 15:33:12,463 - URLValidatorAgent_agent_2 - INFO - [f4a1aa80-fc84-495c-b0fe-6bc3dfefd2d7] Processing task 1/130: https://arxiv.org/abs/2205.05198 (type: article)\n",
      "2025-12-23 15:33:12,464 - URLValidatorAgent_agent_2 - INFO - Starting generic content analysis\n",
      "2025-12-23 15:33:12,464 - URLValidatorAgent_agent_2 - INFO - Starting iteration 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting URL validation pipeline...\n",
      "   Using 2 consumer agents\n",
      "   Max 2 concurrent requests per agent\n",
      "   LM Studio URL: http://localhost:1234\n",
      "üöÄ Starting URL Validation Pipeline\n",
      "   Consumer agents: 2\n",
      "   Max concurrent requests per agent: 2\n",
      "   LM Studio URL: http://localhost:1234\n",
      "\n",
      "üìã Step 1: Creating validation tasks...\n",
      "üîÑ Task Creator Agent starting...\n",
      "üìã Loaded 260 URLs from urls_to_validate.json\n",
      "‚úÖ Created 260 validation tasks\n",
      "üíæ Saved tasks to validation_tasks.json\n",
      "\n",
      "üìä Task Summary:\n",
      "   Total tasks: 260\n",
      "   Tasks by type:\n",
      "     article: 62\n",
      "     paper: 26\n",
      "     video: 61\n",
      "     exercise: 65\n",
      "     python: 37\n",
      "     cpp: 9\n",
      "\n",
      "ü§ñ Step 2: Starting consumer agents...\n",
      "   üü¢ Agent agent_1 starting with 130 tasks\n",
      "üîç Processing task: https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
      "   üü¢ Agent agent_2 starting with 130 tasks\n",
      "üîç Processing task: https://arxiv.org/abs/2205.05198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:15,539 - URLValidatorAgent_agent_1 - INFO - Iteration 1 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:15,540 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:15,540 - URLValidatorAgent_agent_1 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:16,340 - URLValidatorAgent_agent_1 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:33:16,341 - URLValidatorAgent_agent_1 - INFO - Tool firecrawl_scrape completed in 0.80s\n",
      "2025-12-23 15:33:16,341 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:16,341 - URLValidatorAgent_agent_1 - INFO - Starting iteration 2/10\n",
      "2025-12-23 15:33:18,333 - URLValidatorAgent_agent_2 - INFO - Iteration 1 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:18,333 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:18,334 - URLValidatorAgent_agent_2 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:19,064 - URLValidatorAgent_agent_2 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:33:19,065 - URLValidatorAgent_agent_2 - INFO - Tool firecrawl_scrape completed in 0.73s\n",
      "2025-12-23 15:33:19,066 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:19,066 - URLValidatorAgent_agent_2 - INFO - Starting iteration 2/10\n",
      "2025-12-23 15:33:21,450 - URLValidatorAgent_agent_1 - INFO - Iteration 2 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:21,451 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:21,452 - URLValidatorAgent_agent_1 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:21,720 - URLValidatorAgent_agent_1 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:33:21,721 - URLValidatorAgent_agent_1 - INFO - Tool firecrawl_scrape completed in 0.27s\n",
      "2025-12-23 15:33:21,722 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:21,722 - URLValidatorAgent_agent_1 - INFO - Starting iteration 3/10\n",
      "2025-12-23 15:33:25,987 - URLValidatorAgent_agent_2 - INFO - Iteration 2 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:25,988 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:25,988 - URLValidatorAgent_agent_2 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:26,250 - URLValidatorAgent_agent_2 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:33:26,251 - URLValidatorAgent_agent_2 - INFO - Tool firecrawl_scrape completed in 0.26s\n",
      "2025-12-23 15:33:26,253 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:26,253 - URLValidatorAgent_agent_2 - INFO - Starting iteration 3/10\n",
      "2025-12-23 15:33:30,175 - URLValidatorAgent_agent_1 - INFO - Iteration 3 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:30,177 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:30,177 - URLValidatorAgent_agent_1 - INFO - Executing tool: curl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: curl - Using curl tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:32,551 - URLValidatorAgent_agent_1 - INFO - Tool curl completed in 2.37s\n",
      "2025-12-23 15:33:32,553 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:32,553 - URLValidatorAgent_agent_1 - INFO - Forcing final conclusion after successful tool results\n",
      "2025-12-23 15:33:32,554 - URLValidatorAgent_agent_1 - INFO - [e177a8d8-b909-4d79-8ac5-9adf3c1d9b9d] Analysis completed in 20.10s\n",
      "2025-12-23 15:33:32,554 - URLValidatorAgent_agent_1 - INFO - [e177a8d8-b909-4d79-8ac5-9adf3c1d9b9d] Result: valid=True, replacement=False\n",
      "2025-12-23 15:33:32,558 - URLValidatorAgent_agent_1 - INFO - [26c26581-36ec-4be7-b00e-0c3675070320] Processing task 2/130: https://dl.acm.org/doi/10.1145/1365490.1365500 (type: paper)\n",
      "2025-12-23 15:33:32,559 - URLValidatorAgent_agent_1 - INFO - Starting generic content analysis\n",
      "2025-12-23 15:33:32,559 - URLValidatorAgent_agent_1 - INFO - Starting iteration 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing task: https://dl.acm.org/doi/10.1145/1365490.1365500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:35,328 - URLValidatorAgent_agent_2 - INFO - Iteration 3 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:35,329 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:35,330 - URLValidatorAgent_agent_2 - INFO - Executing tool: curl\n",
      "2025-12-23 15:33:35,423 - URLValidatorAgent_agent_2 - INFO - Tool curl completed in 0.09s\n",
      "2025-12-23 15:33:35,424 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:35,425 - URLValidatorAgent_agent_2 - INFO - Forcing final conclusion after successful tool results\n",
      "2025-12-23 15:33:35,425 - URLValidatorAgent_agent_2 - INFO - [f4a1aa80-fc84-495c-b0fe-6bc3dfefd2d7] Analysis completed in 22.96s\n",
      "2025-12-23 15:33:35,426 - URLValidatorAgent_agent_2 - INFO - [f4a1aa80-fc84-495c-b0fe-6bc3dfefd2d7] Result: valid=True, replacement=False\n",
      "2025-12-23 15:33:35,426 - URLValidatorAgent_agent_2 - INFO - [d3249a9c-c856-438d-8916-0c1996c8db4a] Processing task 2/130: https://www.youtube.com/watch?v=0QwZ9BtVu0E (type: video)\n",
      "2025-12-23 15:33:35,427 - URLValidatorAgent_agent_2 - INFO - Starting generic content analysis\n",
      "2025-12-23 15:33:35,428 - URLValidatorAgent_agent_2 - INFO - Starting iteration 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: curl - Using curl tool as requested by LLM\n",
      "üîç Processing task: https://www.youtube.com/watch?v=0QwZ9BtVu0E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:39,375 - URLValidatorAgent_agent_1 - INFO - Iteration 1 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:39,377 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:39,378 - URLValidatorAgent_agent_1 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:40,224 - URLValidatorAgent_agent_1 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:33:40,225 - URLValidatorAgent_agent_1 - INFO - Tool firecrawl_scrape completed in 0.85s\n",
      "2025-12-23 15:33:40,225 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:40,226 - URLValidatorAgent_agent_1 - INFO - Starting iteration 2/10\n",
      "2025-12-23 15:33:42,608 - URLValidatorAgent_agent_2 - INFO - Iteration 1 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:42,609 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:42,610 - URLValidatorAgent_agent_2 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:43,345 - URLValidatorAgent_agent_2 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:33:43,345 - URLValidatorAgent_agent_2 - INFO - Tool firecrawl_scrape completed in 0.73s\n",
      "2025-12-23 15:33:43,346 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:43,346 - URLValidatorAgent_agent_2 - INFO - Starting iteration 2/10\n",
      "2025-12-23 15:33:47,805 - URLValidatorAgent_agent_1 - INFO - Iteration 2 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:47,806 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:47,806 - URLValidatorAgent_agent_1 - INFO - Executing tool: curl\n",
      "2025-12-23 15:33:47,870 - URLValidatorAgent_agent_1 - INFO - Tool curl completed in 0.06s\n",
      "2025-12-23 15:33:47,871 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:47,872 - URLValidatorAgent_agent_1 - INFO - Starting iteration 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: curl - Using curl tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:57,085 - URLValidatorAgent_agent_2 - INFO - Iteration 2 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:33:57,086 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:33:57,086 - URLValidatorAgent_agent_2 - INFO - Executing tool: curl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: curl - Using curl tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:33:57,635 - URLValidatorAgent_agent_2 - INFO - Tool curl completed in 0.55s\n",
      "2025-12-23 15:33:57,638 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:33:57,639 - URLValidatorAgent_agent_2 - INFO - Forcing final conclusion after successful tool results\n",
      "2025-12-23 15:33:57,640 - URLValidatorAgent_agent_2 - INFO - [d3249a9c-c856-438d-8916-0c1996c8db4a] Analysis completed in 22.21s\n",
      "2025-12-23 15:33:57,640 - URLValidatorAgent_agent_2 - INFO - [d3249a9c-c856-438d-8916-0c1996c8db4a] Result: valid=True, replacement=False\n",
      "2025-12-23 15:33:57,650 - URLValidatorAgent_agent_2 - INFO - [6ab14ab5-c7ec-4bd0-8c62-4b3c5a9dff69] Processing task 3/130: https://github.com/NVIDIA/Megatron-LM (type: exercise)\n",
      "2025-12-23 15:33:57,651 - URLValidatorAgent_agent_2 - INFO - Starting generic content analysis\n",
      "2025-12-23 15:33:57,653 - URLValidatorAgent_agent_2 - INFO - Starting iteration 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing task: https://github.com/NVIDIA/Megatron-LM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:06,308 - URLValidatorAgent_agent_1 - INFO - Iteration 3 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:06,309 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:06,310 - URLValidatorAgent_agent_1 - INFO - Executing tool: firecrawl_search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_search - Using firecrawl_search tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:07,065 - URLValidatorAgent_agent_1 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:34:07,066 - URLValidatorAgent_agent_1 - INFO - Tool firecrawl_search completed in 0.76s\n",
      "2025-12-23 15:34:07,067 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:07,067 - URLValidatorAgent_agent_1 - INFO - Starting iteration 4/10\n",
      "2025-12-23 15:34:10,518 - URLValidatorAgent_agent_2 - INFO - Iteration 1 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:10,520 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:10,521 - URLValidatorAgent_agent_2 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:11,324 - URLValidatorAgent_agent_2 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:34:11,325 - URLValidatorAgent_agent_2 - INFO - Tool firecrawl_scrape completed in 0.80s\n",
      "2025-12-23 15:34:11,325 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:11,326 - URLValidatorAgent_agent_2 - INFO - Starting iteration 2/10\n",
      "2025-12-23 15:34:15,261 - URLValidatorAgent_agent_1 - INFO - Iteration 4 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:15,262 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:15,262 - URLValidatorAgent_agent_1 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:15,519 - URLValidatorAgent_agent_1 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:34:15,521 - URLValidatorAgent_agent_1 - INFO - Tool firecrawl_scrape completed in 0.26s\n",
      "2025-12-23 15:34:15,521 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:15,522 - URLValidatorAgent_agent_1 - INFO - Starting iteration 5/10\n",
      "2025-12-23 15:34:20,541 - URLValidatorAgent_agent_2 - INFO - Iteration 2 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:20,542 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:20,542 - URLValidatorAgent_agent_2 - INFO - Executing tool: curl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: curl - Using curl tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:21,362 - URLValidatorAgent_agent_2 - INFO - Tool curl completed in 0.82s\n",
      "2025-12-23 15:34:21,365 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:21,365 - URLValidatorAgent_agent_2 - INFO - Forcing final conclusion after successful tool results\n",
      "2025-12-23 15:34:21,369 - URLValidatorAgent_agent_2 - INFO - [6ab14ab5-c7ec-4bd0-8c62-4b3c5a9dff69] Analysis completed in 23.72s\n",
      "2025-12-23 15:34:21,370 - URLValidatorAgent_agent_2 - INFO - [6ab14ab5-c7ec-4bd0-8c62-4b3c5a9dff69] Result: valid=False, replacement=False\n",
      "2025-12-23 15:34:21,375 - URLValidatorAgent_agent_2 - INFO - [042e86f2-00ff-4735-ba1f-0275d0d23c9f] Processing task 4/130: https://github.com/NVIDIA/Megatron-LM (type: exercise)\n",
      "2025-12-23 15:34:21,376 - URLValidatorAgent_agent_2 - INFO - Starting generic content analysis\n",
      "2025-12-23 15:34:21,377 - URLValidatorAgent_agent_2 - INFO - Starting iteration 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing task: https://github.com/NVIDIA/Megatron-LM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:25,118 - URLValidatorAgent_agent_1 - INFO - Iteration 5 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:25,119 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:25,119 - URLValidatorAgent_agent_1 - INFO - Executing tool: curl\n",
      "2025-12-23 15:34:25,190 - URLValidatorAgent_agent_1 - INFO - Tool curl completed in 0.07s\n",
      "2025-12-23 15:34:25,191 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:25,191 - URLValidatorAgent_agent_1 - INFO - Starting iteration 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: curl - Using curl tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:35,196 - URLValidatorAgent_agent_2 - INFO - Iteration 1 - Action: continue, Tool calls: 0\n",
      "2025-12-23 15:34:35,198 - URLValidatorAgent_agent_2 - INFO - Starting iteration 2/10\n",
      "2025-12-23 15:34:45,666 - URLValidatorAgent_agent_1 - INFO - Iteration 6 - Action: continue, Tool calls: 0\n",
      "2025-12-23 15:34:45,667 - URLValidatorAgent_agent_1 - INFO - Starting iteration 7/10\n",
      "2025-12-23 15:34:49,868 - URLValidatorAgent_agent_2 - INFO - Iteration 2 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:49,869 - URLValidatorAgent_agent_2 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:49,869 - URLValidatorAgent_agent_2 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:50,609 - URLValidatorAgent_agent_2 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:34:50,610 - URLValidatorAgent_agent_2 - INFO - Tool firecrawl_scrape completed in 0.74s\n",
      "2025-12-23 15:34:50,610 - URLValidatorAgent_agent_2 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:50,611 - URLValidatorAgent_agent_2 - INFO - Starting iteration 3/10\n",
      "2025-12-23 15:34:55,672 - URLValidatorAgent_agent_1 - INFO - Iteration 7 - Action: use_tools, Tool calls: 1\n",
      "2025-12-23 15:34:55,673 - URLValidatorAgent_agent_1 - INFO - Executing 1 tool calls\n",
      "2025-12-23 15:34:55,673 - URLValidatorAgent_agent_1 - INFO - Executing tool: firecrawl_scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Executing tool: firecrawl_scrape - Using firecrawl_scrape tool as requested by LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 15:34:56,425 - URLValidatorAgent_agent_1 - ERROR - Firecrawl API error 401: {\"success\":false,\"error\":\"Unauthorized: Invalid token\"}\n",
      "2025-12-23 15:34:56,426 - URLValidatorAgent_agent_1 - INFO - Tool firecrawl_scrape completed in 0.75s\n",
      "2025-12-23 15:34:56,427 - URLValidatorAgent_agent_1 - INFO - Completed executing 1 tools\n",
      "2025-12-23 15:34:56,427 - URLValidatorAgent_agent_1 - INFO - Starting iteration 8/10\n",
      "2025-12-23 15:35:05,879 - URLValidatorAgent_agent_2 - INFO - Iteration 3 - Action: continue, Tool calls: 0\n",
      "2025-12-23 15:35:05,880 - URLValidatorAgent_agent_2 - INFO - Starting iteration 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Agent agent_1 was cancelled, attempting to save partial results...\n",
      "‚ö†Ô∏è  Agent agent_2 was cancelled, attempting to save partial results...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Run validation pipeline\u001b[39;00m\n\u001b[32m     17\u001b[39m start_time = datetime.now()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m summary = \u001b[38;5;28;01mawait\u001b[39;00m orchestrator.run_validation_pipeline()\n\u001b[32m     19\u001b[39m end_time = datetime.now()\n\u001b[32m     21\u001b[39m duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/url_validator/url_validation_orchestrator.py:62\u001b[39m, in \u001b[36mURLValidationOrchestrator.run_validation_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Step 2: Distribute tasks to consumer agents\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mü§ñ Step 2: Starting consumer agents...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_consumer_agents(tasks)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Step 3: Generate summary (results already saved incrementally)\u001b[39;00m\n\u001b[32m     65\u001b[39m summary = \u001b[38;5;28mself\u001b[39m._generate_summary()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/url_validator/url_validation_orchestrator.py:100\u001b[39m, in \u001b[36mURLValidationOrchestrator._run_consumer_agents\u001b[39m\u001b[34m(self, tasks)\u001b[39m\n\u001b[32m     96\u001b[39m     agent_coroutines.append(coroutine)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Use return_exceptions=True to prevent cancellation propagation\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# This ensures that if one agent fails or gets cancelled, others continue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m agent_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*agent_coroutines, return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Handle any exceptions that occurred\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agent_results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/url_validator/url_validation_orchestrator.py:126\u001b[39m, in \u001b[36mURLValidationOrchestrator._run_single_agent\u001b[39m\u001b[34m(self, agent_id, tasks)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m GenericContentValidatorAgent(\n\u001b[32m    121\u001b[39m         agent_id=agent_id,\n\u001b[32m    122\u001b[39m         lm_studio_url=\u001b[38;5;28mself\u001b[39m.lm_studio_url,\n\u001b[32m    123\u001b[39m         max_concurrent_requests=\u001b[38;5;28mself\u001b[39m.max_concurrent_requests_per_agent\n\u001b[32m    124\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m agent:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         processed_tasks = \u001b[38;5;28;01mawait\u001b[39;00m agent.process_tasks(tasks)\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# Thread-safe update of completed tasks\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/url_validator/url_validator_agent.py:997\u001b[39m, in \u001b[36mGenericContentValidatorAgent.process_tasks\u001b[39m\u001b[34m(self, tasks)\u001b[39m\n\u001b[32m    995\u001b[39m \u001b[38;5;66;03m# Use generic analysis\u001b[39;00m\n\u001b[32m    996\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m analysis_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.analyze_content_generic(content_request)\n\u001b[32m    998\u001b[39m end_time = time.time()\n\u001b[32m   1000\u001b[39m analysis_duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/url_validator/url_validator_agent.py:867\u001b[39m, in \u001b[36mGenericContentValidatorAgent.analyze_content_generic\u001b[39m\u001b[34m(self, content_request)\u001b[39m\n\u001b[32m    863\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prompt created (length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chars)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    866\u001b[39m     \u001b[38;5;66;03m# Get LLM response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m     llm_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_llm(prompt)\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Parse LLM response for tool usage and reasoning\u001b[39;00m\n\u001b[32m    870\u001b[39m     action, reasoning, tool_calls = \u001b[38;5;28mself\u001b[39m._parse_llm_response(llm_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/url_validator/url_validator_agent.py:311\u001b[39m, in \u001b[36mGenericContentValidatorAgent._call_llm\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m    299\u001b[39m request_data = {\n\u001b[32m    300\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-oss-20b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    301\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.temperature\n\u001b[32m    308\u001b[39m }\n\u001b[32m    309\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM request data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(request_data,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.session.post(\n\u001b[32m    312\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lm_studio_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    313\u001b[39m     json=request_data\n\u001b[32m    314\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status == \u001b[32m200\u001b[39m:\n\u001b[32m    316\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/gpu_ladder_env/lib64/python3.14/site-packages/aiohttp/client.py:1510\u001b[39m, in \u001b[36m_BaseRequestContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1509\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _RetType:\n\u001b[32m-> \u001b[39m\u001b[32m1510\u001b[39m     \u001b[38;5;28mself\u001b[39m._resp: _RetType = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coro\n\u001b[32m   1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._resp.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/gpu_ladder_env/lib64/python3.14/site-packages/aiohttp/client.py:779\u001b[39m, in \u001b[36mClientSession._request\u001b[39m\u001b[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[39m\n\u001b[32m    776\u001b[39m     handler = _connect_and_send_request\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m handler(req)\n\u001b[32m    780\u001b[39m \u001b[38;5;66;03m# Client connector errors should not be retried\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    782\u001b[39m     ConnectionTimeoutError,\n\u001b[32m    783\u001b[39m     ClientConnectorError,\n\u001b[32m    784\u001b[39m     ClientConnectorCertificateError,\n\u001b[32m    785\u001b[39m     ClientConnectorSSLError,\n\u001b[32m    786\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/gpu_ladder_env/lib64/python3.14/site-packages/aiohttp/client.py:757\u001b[39m, in \u001b[36mClientSession._request.<locals>._connect_and_send_request\u001b[39m\u001b[34m(req)\u001b[39m\n\u001b[32m    755\u001b[39m resp = \u001b[38;5;28;01mawait\u001b[39;00m req.send(conn)\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m resp.start(conn)\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    759\u001b[39m     resp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/gpu_ladder_env/lib64/python3.14/site-packages/aiohttp/client_reqrep.py:539\u001b[39m, in \u001b[36mClientResponse.start\u001b[39m\u001b[34m(self, connection)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    538\u001b[39m     protocol = \u001b[38;5;28mself\u001b[39m._protocol\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     message, payload = \u001b[38;5;28;01mawait\u001b[39;00m protocol.read()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m http.HttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m    542\u001b[39m         \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m    543\u001b[39m         \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         headers=exc.headers,\n\u001b[32m    547\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gpu-programming-ladder/gpu_ladder_env/lib64/python3.14/site-packages/aiohttp/streams.py:680\u001b[39m, in \u001b[36mDataQueue.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;28mself\u001b[39m._waiter = \u001b[38;5;28mself\u001b[39m._loop.create_future()\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._waiter\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio.CancelledError, asyncio.TimeoutError):\n\u001b[32m    682\u001b[39m     \u001b[38;5;28mself\u001b[39m._waiter = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run URL validation with orchestrator\n",
    "print(\"üöÄ Starting URL validation pipeline...\")\n",
    "print(f\"   Using {CONFIG['num_consumer_agents']} consumer agents\")\n",
    "print(f\"   Max {CONFIG['max_concurrent_requests_per_agent']} concurrent requests per agent\")\n",
    "print(f\"   LM Studio URL: {CONFIG['lm_studio_url']}\")\n",
    "\n",
    "try:\n",
    "    orchestrator = URLValidationOrchestrator(\n",
    "        num_consumer_agents=CONFIG['num_consumer_agents'],\n",
    "        max_concurrent_requests_per_agent=CONFIG['max_concurrent_requests_per_agent'],\n",
    "        lm_studio_url=CONFIG['lm_studio_url'],\n",
    "        tasks_file=CONFIG['tasks_file'],\n",
    "        results_file=CONFIG['results_file']\n",
    "    )\n",
    "    \n",
    "    # Run validation pipeline\n",
    "    start_time = datetime.now()\n",
    "    summary = await orchestrator.run_validation_pipeline()\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    print(f\"\\n‚è±Ô∏è  Validation completed in {duration.total_seconds():.1f} seconds\")\n",
    "    \n",
    "    # Add timing information to summary for enhanced reporting\n",
    "    if 'error' not in summary:\n",
    "        summary['total_duration_seconds'] = duration.total_seconds()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during validation: {e}\")\n",
    "    summary = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Review Validation Results\n",
    "\n",
    "Review the validation results and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation summary\n",
    "if 'error' not in summary:\n",
    "    print(\"üìä Validation Summary:\")\n",
    "    print(f\"   Total URLs processed: {summary['total_urls_processed']}\")\n",
    "    print(f\"   Valid URLs: {summary['valid_urls']}\")\n",
    "    print(f\"   Invalid URLs: {summary['invalid_urls']}\")\n",
    "    print(f\"   URLs with replacements: {summary['replaced_urls']}\")\n",
    "    print(f\"   URLs to be removed: {summary['removed_urls']}\")\n",
    "    print(f\"   Success rate: {summary['success_rate']:.1f}%\")\n",
    "    \n",
    "    # Time-related statistics\n",
    "    if 'total_duration_seconds' in summary and summary['total_urls_processed'] > 0:\n",
    "        avg_time_per_url = summary['total_duration_seconds'] / summary['total_urls_processed']\n",
    "        urls_per_minute = (summary['total_urls_processed'] / summary['total_duration_seconds']) * 60\n",
    "        \n",
    "        print(\"\\n‚è±Ô∏è  Performance Statistics:\")\n",
    "        print(f\"   Total processing time: {summary['total_duration_seconds']:.1f} seconds\")\n",
    "        print(f\"   Average time per URL: {avg_time_per_url:.2f} seconds\")\n",
    "        print(f\"   Processing rate: {urls_per_minute:.1f} URLs/minute\")\n",
    "        \n",
    "        # Phase breakdown if available\n",
    "        if 'phase_durations' in summary:\n",
    "            phases = summary['phase_durations']\n",
    "            print(\"\\nüìà Time Breakdown by Phase:\")\n",
    "            for phase, duration in phases.items():\n",
    "                percentage = (duration / summary['total_duration_seconds']) * 100\n",
    "                print(f\"   {phase}: {duration:.1f}s ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüìã Breakdown by URL type:\")\n",
    "    for url_type, stats in summary['urls_by_type'].items():\n",
    "        total = stats['total']\n",
    "        valid = stats['valid']\n",
    "        replaced = stats['replaced']\n",
    "        removed = stats['invalid'] - stats['replaced']\n",
    "        \n",
    "        # Time stats per URL type if available\n",
    "        type_stats = f\"{url_type}: {valid}/{total} valid, {replaced} replaced, {removed} to remove\"\n",
    "        if 'avg_time_per_url' in stats:\n",
    "            type_stats += f\" (avg: {stats['avg_time_per_url']:.2f}s/URL)\"\n",
    "        print(f\"   {type_stats}\")\n",
    "        \n",
    "    # Show agent performance if available\n",
    "    if 'agent_performance' in summary:\n",
    "        print(\"\\nü§ñ Agent Performance:\")\n",
    "        for agent_id, perf in summary['agent_performance'].items():\n",
    "            urls_processed = perf.get('urls_processed', 0)\n",
    "            time_spent = perf.get('time_spent', 0)\n",
    "            avg_time = time_spent / urls_processed if urls_processed > 0 else 0\n",
    "            print(f\"   Agent {agent_id}: {urls_processed} URLs, {time_spent:.1f}s total, {avg_time:.2f}s/URL\")\n",
    "else:\n",
    "    print(f\"‚ùå Validation failed: {summary['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update data.js File (Optional)\n",
    "\n",
    "Update the original data.js file with the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data.js with validation results\n",
    "if CONFIG['update_data_js'] and 'error' not in summary:\n",
    "    print(\"üîÑ Updating data.js with validation results...\")\n",
    "    \n",
    "    try:\n",
    "        success = await orchestrator.update_data_js_with_results(CONFIG['data_js_file'])\n",
    "        if success:\n",
    "            print(\"‚úÖ data.js updated successfully\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to update data.js\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating data.js: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping data.js update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Tools Configuration\n",
    "\n",
    "To configure MCP (Model Context Protocol) tools for enhanced functionality:\n",
    "\n",
    "### Available MCP Tools\n",
    "1. **firecrawl**: Web scraping and content extraction\n",
    "2. **context7**: Library documentation search\n",
    "3. **brave-search**: Web search capabilities\n",
    "\n",
    "### Configuration Steps\n",
    "1. Install MCP server packages\n",
    "2. Configure MCP client in your LLM setup\n",
    "3. Update the `find_replacement_url` method to use MCP tools\n",
    "\n",
    "### Example MCP Integration\n",
    "```python\n",
    "# Add to URLValidatorAgent.__init__\n",
    "self.mcp_client = MCPClient(\n",
    "    server_configs={\n",
    "        'firecrawl': {'url': 'http://localhost:3000'},\n",
    "        'context7': {'url': 'http://localhost:3001'}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use in find_replacement_url method\n",
    "search_results = await self.mcp_client.search('brave-search', query)\n",
    "scraped_content = await self.mcp_client.scrape('firecrawl', url)\n",
    "```\n",
    "\n",
    "### Benefits of MCP Integration\n",
    "- **Enhanced search**: Use Brave Search for finding replacements\n",
    "- **Better scraping**: Use Firecrawl for content validation\n",
    "- **Documentation lookup**: Use Context7 for library-specific resources\n",
    "- **Fallback mechanisms**: Multiple tools for robust URL finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**LM Studio Connection Issues**\n",
    "```bash\n",
    "# Check LM Studio status\n",
    "curl http://localhost:1234/v1/models\n",
    "\n",
    "# Verify model is loaded\n",
    "# Restart LM Studio if needed\n",
    "```\n",
    "\n",
    "**Rate Limiting**\n",
    "```json\n",
    "// Reduce concurrent requests in config.json\n",
    "{\n",
    "  \"agents\": {\n",
    "    \"max_concurrent_requests_per_agent\": 3\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Memory Issues**\n",
    "```json\n",
    "// Process in smaller batches\n",
    "{\n",
    "  \"processing\": {\n",
    "    \"batch_size\": 25\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**URL Validation Issues**\n",
    "- Some sites block automated requests - consider using proxies\n",
    "- PDFs and binary content may not be properly analyzed\n",
    "- GitHub rate limiting may affect repository checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this validation system:\n",
    "\n",
    "1. **Review changes**: Check the updated data.js file\n",
    "2. **Manual verification**: Spot-check some URLs to ensure replacements are appropriate\n",
    "3. **Re-run periodically**: URLs can break over time, so re-validation is recommended\n",
    "4. **Extend functionality**: Add more URL types or validation rules as needed\n",
    "\n",
    "### Potential Enhancements\n",
    "- **Content freshness checking**: Verify that content is still relevant\n",
    "- **Duplicate detection**: Find and remove duplicate URLs\n",
    "- **Quality scoring**: Rate URLs by content quality\n",
    "- **Automated scheduling**: Set up regular validation runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
